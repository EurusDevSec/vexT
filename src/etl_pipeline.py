import pandas as pd
from sentence_transformers import SentenceTransformer
import os

# --- C·∫§U H√åNH D·ªÆ LI·ªÜU TH·ª∞C (REAL WORLD CONFIG) ---
# 1. ƒê∆∞·ªùng d·∫´n file CSV t·∫£i t·ª´ Kaggle (ƒê·∫∑t file n√†y v√†o th∆∞ m·ª•c res/)
# V√≠ d·ª• b·∫°n t·∫£i file t√™n l√† 'flipkart_com-ecommerce_sample.csv'
CSV_FILENAME = "flipkart_data.csv" 

# 2. B·∫¢N ƒê·ªí √ÅNH X·∫† C·ªòT (MAPPING SCHEMA)
# B√™n Tr√°i: T√™n c·ªôt trong h·ªá th·ªëng VexT (C·ªê ƒê·ªäNH)
# B√™n Ph·∫£i: T√™n c·ªôt trong file CSV t·∫£i v·ªÅ (THAY ƒê·ªîI T√ôY FILE)
COLUMN_MAPPING = {
    "title": "product_name",        # Trong CSV Kaggle c·ªôt t√™n l√† product_name
    "price": "retail_price",        # Trong CSV Kaggle c·ªôt t√™n l√† retail_price
    "category": "product_category_tree", 
    "content_text": "description",  # C·ªôt m√¥ t·∫£ d√πng ƒë·ªÉ t·∫°o vector
    "publish_date": "crawl_timestamp" # Ng√†y th√°ng (n·∫øu c√≥)
}

# 3. GI·ªöI H·∫†N D·ªÆ LI·ªÜU (QUAN TR·ªåNG)
# Vector h√≥a t·ªën nhi·ªÅu CPU. ƒê·ªÉ demo m∆∞·ª£t, h√£y gi·ªõi h·∫°n 2000-5000 d√≤ng.
# ƒê·ª´ng tham load c·∫£ 100k d√≤ng n·∫øu kh√¥ng c√≥ GPU.
DATA_LIMIT = 5000 

print("‚è≥ Loading model AI...")
# model = SentenceTransformer('all-MiniLM-L6-v2')
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') # D√πng model ƒëa ng√¥n ng·ªØ ƒë·ªÉ kh·ªõp v·ªõi search_core

def load_and_map_data(file_path):
    print(f"üîÑ ƒêang ƒë·ªçc file Big Data: {file_path}")
    
    # ƒê·ªçc CSV
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file CSV. H√£y t·∫£i t·ª´ Kaggle v√† b·ªè v√†o th∆∞ m·ª•c res/")
        return None

    # ƒê·ªïi t√™n c·ªôt theo Mapping
    # ƒê·∫£o ng∆∞·ª£c dict ƒë·ªÉ d√πng h√†m rename: {T√™n_C≈©: T√™n_M·ªõi}
    rename_dict = {v: k for k, v in COLUMN_MAPPING.items()}
    df = df.rename(columns=rename_dict)
    
    # Ki·ªÉm tra xem c√≥ ƒë·ªß c·ªôt quan tr·ªçng kh√¥ng
    required_cols = ["title", "content_text"]
    for col in required_cols:
        if col not in df.columns:
            print(f"‚ùå File CSV thi·∫øu c·ªôt quan tr·ªçng map v√†o '{col}'. Ki·ªÉm tra l·∫°i COLUMN_MAPPING!")
            return None

    # Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt cho VexT
    available_cols = [c for c in COLUMN_MAPPING.keys() if c in df.columns]
    df = df[available_cols]

    return df

def clean_data(df):
    print(f"üßπ ƒêang l√†m s·∫°ch {len(df)} d√≤ng d·ªØ li·ªáu...")
    
    # 1. Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng (Sampling) nh∆∞ng GI·ªÆ L·∫†I DEMO DATA
    if len(df) > DATA_LIMIT:
        print(f"‚ö†Ô∏è D·ªØ li·ªáu qu√° l·ªõn ({len(df)} d√≤ng).")
        
        # Danh s√°ch t·ª´ kh√≥a quan tr·ªçng cho Demo
        demo_keywords = [
            "Alisha Solid Women's Cycling Shorts",
            "FabHomeDecor Fabric Double Sofa Bed",
            "Sicons All Purpose Arnica Dog Shampoo",
            "AW Bellies",
            "Eternal Gandhi Super Series Crystal Paper Weights"
        ]
        
        # L·ªçc ra c√°c d√≤ng ch·ª©a t·ª´ kh√≥a demo (Case insensitive)
        # T·∫°o mask: N·∫øu title ch·ª©a b·∫•t k·ª≥ keyword n√†o -> True
        mask = df['title'].astype(str).apply(lambda x: any(k.lower() in x.lower() for k in demo_keywords))
        df_demo = df[mask]
        print(f"   üëâ ƒê√£ t√¨m th·∫•y {len(df_demo)} s·∫£n ph·∫©m Demo quan tr·ªçng.")
        
        # L·∫•y ph·∫ßn c√≤n l·∫°i ƒë·ªÉ fill cho ƒë·ªß DATA_LIMIT
        df_rest = df[~mask]
        remaining_count = DATA_LIMIT - len(df_demo)
        
        if remaining_count > 0:
            df_sample = df_rest.sample(n=remaining_count, random_state=42)
            df = pd.concat([df_demo, df_sample])
        else:
            df = df_demo.head(DATA_LIMIT)
            
        print(f"   ‚úÖ ƒê√£ ch·ªët danh s√°ch {len(df)} d√≤ng (Bao g·ªìm Demo Data).")
    
    # 2. X·ª≠ l√Ω Gi√° ti·ªÅn (L·ªçc b·ªè ch·ªØ, ch·ªâ l·∫•y s·ªë)
    # V√≠ d·ª• Kaggle hay ghi gi√° l√† "20,000 USD" -> c·∫ßn chuy·ªÉn th√†nh s·ªë
    if 'price' in df.columns:
        # √âp ki·ªÉu s·ªë, l·ªói th√†nh NaN
        df['price'] = pd.to_numeric(df['price'], errors='coerce')
        df['price'] = df['price'].fillna(0) # Gi√° r·ªóng th√¨ cho b·∫±ng 0
    
    # 3. X·ª≠ l√Ω Category (L√†m s·∫°ch chu·ªói)
    if 'category' in df.columns:
        # L·∫•y danh m·ª•c cha ƒë·∫ßu ti√™n, lo·∫°i b·ªè k√Ω t·ª± th·ª´a
        df['category'] = df['category'].astype(str).apply(lambda x: x.replace('["', '').replace('"]', '').split(">>")[0].strip())
    else:
        df['category'] = "General"

    # 4. X·ª≠ l√Ω Null ·ªü Description
    df = df.dropna(subset=['content_text'])
    df['content_text'] = df['content_text'].astype(str)
    
    # 5. X·ª≠ l√Ω Ng√†y th√°ng (N·∫øu c√≥)
    if 'publish_date' in df.columns:
         df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce')
    
    return df

def generate_vectors(df):
    print(f"üß† ƒêang Vector h√≥a {len(df)} s·∫£n ph·∫©m (Vi·ªác n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    
    sentences = df['content_text'].tolist()
    
    # Batch size = 64 gi√∫p ch·∫°y nhanh h∆°n
    embeddings = model.encode(sentences, batch_size=64, show_progress_bar=True)
    
    df['embedding'] = list(embeddings)
    return df

def main():
    # Setup ƒë∆∞·ªùng d·∫´n
    dir_script = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    input_path = os.path.join(dir_script, "res", CSV_FILENAME)
    output_path = os.path.join(dir_script, "res", "flipkart_data_ready.json")

    # Pipeline
    df = load_and_map_data(input_path)
    if df is not None:
        df_clean = clean_data(df)
        df_final = generate_vectors(df_clean)
        
        # L∆∞u k·∫øt qu·∫£
        df_final.to_json(output_path, orient='records', date_format='iso')
        print(f"\n‚úÖ XONG! ƒê√£ l∆∞u {len(df_final)} s·∫£n ph·∫©m vector h√≥a v√†o: {output_path}")
        print("üëâ B√¢y gi·ªù h√£y ch·∫°y l·∫°i 'uv run search_core.py' ƒë·ªÉ n·∫°p d·ªØ li·ªáu m·ªõi n√†y v√†o OpenSearch!")

if __name__ == "__main__":
    main()